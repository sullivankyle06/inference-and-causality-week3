{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7871310d",
   "metadata": {},
   "source": [
    "# Conditional Independence\n",
    "**Hands‑on Notebook**\n",
    "\n",
    "\n",
    "**In this notebook**\n",
    "Explore **conditional independence** in chain / fork / collider.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a036c571",
   "metadata": {},
   "source": [
    "## 0) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ca1293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rng = np.random.default_rng(7)\n",
    "\n",
    "def summarize_binary(y, x=None, do=None, df=None, name=\"\"):\n",
    "    if df is not None and x is not None:\n",
    "        p = df.loc[df[x]==1, y].mean()\n",
    "        n = (df[x]==1).sum()\n",
    "        print(f\"P({y}=1 | {x}=1) = {p:.3f}  [n={n}]  {name}\")\n",
    "    if df is not None and do is not None:\n",
    "        p = df[y].mean()\n",
    "        n = len(df)\n",
    "        print(f\"P({y}=1 | do({do})) = {p:.3f}  [n={n}]  {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6951d623",
   "metadata": {},
   "source": [
    "\n",
    "## A) Seeing vs Doing with the **Firing Squad** toy model\n",
    "\n",
    "![Firing squad DAG](../images/firing_squad.png)\n",
    "\n",
    "- Variables: \n",
    "`C` (captain order) \n",
    "`SA` (squad A fires)\n",
    "`SB` (squad B fires)\n",
    "`D` (death).  \n",
    "\n",
    "\n",
    "We will compare:\n",
    "- **Observation**: `P(D|SA=0)` — low, because when A doesn't fires, usually B also does not fire (same cause `C`).\n",
    "- **Intervention**: `P(D|do(SA=0))` — set A to not fire regardless of the command `C`; isolate A's own causal contribution. Now we have some times B firind and some times B not firing resulting in a more complex scenario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85456374",
   "metadata": {},
   "source": [
    "To have a bit of more interesting scenario, we impose some imperfection to obediance of our squads in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3cbb40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observational world:\n",
      "P(D=1 | SA=0) = 1.000  [n=52415]  (seeing)\n",
      "\n",
      "Interventional world:\n",
      "P(D=1 | do(SA=0)) = 0.475  [n=100000]  (doing)\n"
     ]
    }
   ],
   "source": [
    "N = 100000\n",
    "rng = np.random.default_rng(6)\n",
    "\n",
    "# Structural equations (binary)\n",
    "C = rng.binomial(1, 0.5, size=N)  # Captain order\n",
    "SA = (C & (rng.random(N) < 0.95)).astype(int)  # Squad A obeys if ordered\n",
    "SB = (C & (rng.random(N) < 0.95)).astype(int)  # Squad B obeys if ordered\n",
    "\n",
    "# Lethality (set to 1 for simplicity, adjust if you want realism)\n",
    "p_kill_A, p_kill_B = 1.0, 1.0\n",
    "hit_A = p_kill_A\n",
    "hit_B = p_kill_B\n",
    "# hit_A = (SA & (rng.random(N) < p_kill_A)).astype(int)\n",
    "# hit_B = (SB & (rng.random(N) < p_kill_B)).astype(int)\n",
    "D = np.maximum(hit_A, hit_B)\n",
    "\n",
    "# Combine all simulated variables into a single DataFrame for easier analysis and plotting\n",
    "obs_df = pd.DataFrame(dict(C=C, SA=SA, SB=SB, D=D))\n",
    "\n",
    "print(\"Observational world:\")\n",
    "# Compute P(D=1 | SA=0)\n",
    "p_obs = obs_df.loc[obs_df[\"SA\"] == 0, \"D\"].mean()\n",
    "n_obs = (obs_df[\"SA\"] == 0).sum()\n",
    "print(f\"P(D=1 | SA=0) = {p_obs:.3f}  [n={n_obs}]  (seeing)\")\n",
    "\n",
    "# --- Interventional: do(SA=0) ---\n",
    "C2 = rng.binomial(1, 0.5, size=N)                   # captain as before\n",
    "SA2 = np.zeros(N, dtype=int)                        # force A not to fire\n",
    "SB2 = (C2 & (rng.random(N) < 0.95)).astype(int)     # B still reacts to captain\n",
    "\n",
    "hit_A2 = (SA2 & (rng.random(N) < p_kill_A)).astype(int)\n",
    "hit_B2 = (SB2 & (rng.random(N) < p_kill_B)).astype(int)\n",
    "D2 = np.maximum(hit_A2, hit_B2)\n",
    "\n",
    "do_df = pd.DataFrame(dict(C=C2, SA=SA2, SB=SB2, D=D2))\n",
    "p_do = do_df[\"D\"].mean()\n",
    "n_do = len(do_df)\n",
    "\n",
    "print(\"\\nInterventional world:\")\n",
    "print(f\"P(D=1 | do(SA=0)) = {p_do:.3f}  [n={n_do}]  (doing)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c84796",
   "metadata": {},
   "source": [
    "Number n above shows the number of tests where SA=0 happened. When we only observed, about half of the time, SA=0 and when we intervened, it was always kept at 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a4d4d9",
   "metadata": {},
   "source": [
    "#### We see that P(D=1 | SA=0) ≠ P(D=1 | do(SA=0))!\n",
    "This difference reveals that **C (the captain’s order)** is a **confounder** — it influences both the squad’s action (`SA`) and the outcome (`D`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ab55b5",
   "metadata": {},
   "source": [
    "## B) Conditional Independence in **Chain / Fork / Collider**\n",
    "\n",
    "In this section we simulate three fundamental causal structures (often called the *building blocks* of causal graphs)  \n",
    "to explore how **conditional independence** behaves in each.\n",
    "\n",
    "Reminder:\n",
    "### Marginal vs Conditional Correlation\n",
    "\n",
    "- **Marginal correlation** measures how two variables vary together *overall*, without taking any other variables into account.  \n",
    "  → Example: the raw relationship between Smoking and Cancer in the population.\n",
    "\n",
    "- **Conditional correlation** measures how two variables relate *after we fix or control for* a third variable.  \n",
    "  → Example: the relationship between Smoking and Cancer **within each level of Tar exposure**.\n",
    "\n",
    "**Key idea:**  \n",
    "If two variables are correlated marginally but not conditionally, it means a third variable (a mediator or confounder) explains their association.  \n",
    "Conversely, if they are independent marginally but correlated conditionally, conditioning has **opened a path** (as in collider bias).\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Chain: A → B → C\n",
    "**Interpretation:**  \n",
    "B, \"the mediator\" transmits information or influence from A to C.  \n",
    "- *Example:* Smoking → Tar in lungs → Cancer.  \n",
    "- A and C are correlated because information “flows” through B.  \n",
    "- **If we condition on B**, we block that path — A and C become (approximately) independent.\n",
    "\n",
    "**Expectation:**  \n",
    "- Marginal correlation: high (A and C move together).  \n",
    "- Conditional correlation given B: ≈ 0 (path blocked).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Fork (Confounding): A ← U → C\n",
    "**Interpretation:**  \n",
    "U is a *common cause* (confounder) of both A and C.  \n",
    "- *Example:* Genetic predisposition → Smoking and Cancer.  \n",
    "- A and C appear correlated, but only because of U.  \n",
    "- **If we condition on U**, we remove that shared cause and eliminate the spurious correlation.\n",
    "\n",
    "**Expectation:**  \n",
    "- Marginal correlation: high (U induces a false link).  \n",
    "- Conditional correlation given U: ≈ 0 (confounding removed).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Collider: A → B ← C\n",
    "**Interpretation:**  \n",
    "B is a *common effect* (collider) of A and C.  \n",
    "- *Example:*  \n",
    "  - A = Smoking  \n",
    "  - C = Air pollution  \n",
    "  - B = Hospital admission (caused by either).  \n",
    "- Normally, A and C are independent.  \n",
    "- **If we condition on B** (or any descendant of B), we *create* a correlation between A and C —  \n",
    "  this is known as **collider bias** or **selection bias**.\n",
    "\n",
    "**Expectation:**  \n",
    "- Marginal correlation: near 0 (A, C independent).  \n",
    "- Conditional correlation given B: strong (conditioning opens the path).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Conclusion\n",
    "> Correlation alone can mislead: depending on the graph, conditioning can remove, reveal, or even **fabricate** relationships.\n",
    ">\n",
    "> Understanding which paths are open or closed (via **d-separation**) is central to causal inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7349a779",
   "metadata": {},
   "source": [
    "\n",
    "### What the following block of code does\n",
    "- Each function (`sim_chain`, `sim_fork`, `sim_collider`) simulates random data following these causal relationships.  \n",
    "- We compute:\n",
    "  - **Marginal correlation**: `corr(A, C)`  \n",
    "  - **Conditional correlation**: `corr(A, C | middle node)` using simple binning on the conditioning variable.\n",
    "- This shows how *conditioning* can either **block** or **create** associations depending on the graph structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "069adc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain: corr(A,C)  (marginal) = 0.5801684286913067\n",
      "Fork:  corr(A,C)  (marginal) = 0.49713950788544\n",
      "Collider: corr(A,C) (marginal) = -0.01064898968845041\n",
      "\n",
      "Conditioning (approx via binning):\n",
      "Chain: corr(A,C | B) ≈ 0.047197088807209964\n",
      "Fork:  corr(A,C | U) ≈ 0.03878851215357192\n",
      "Collider: corr(A,C | B) ≈ -0.4745972035238439\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def sim_chain(N=50_000, seed=1):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    A = rng.normal(0,1,N)\n",
    "    B = A + rng.normal(0,1,N)\n",
    "    C = B + rng.normal(0,1,N)\n",
    "    return pd.DataFrame(dict(A=A,B=B,C=C))\n",
    "\n",
    "def sim_fork(N=50_000, seed=2):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    U = rng.normal(0,1,N)\n",
    "    A = U + rng.normal(0,1,N)\n",
    "    C = U + rng.normal(0,1,N)\n",
    "    return pd.DataFrame(dict(U=U,A=A,C=C))\n",
    "\n",
    "def sim_collider(N=50_000, seed=3):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    A = rng.normal(0,1,N)\n",
    "    C = rng.normal(0,1,N)\n",
    "    B = A + C + rng.normal(0,1,N)  # collider\n",
    "    return pd.DataFrame(dict(A=A,B=B,C=C))\n",
    "\n",
    "def corr(x,y,df):\n",
    "    return np.corrcoef(df[x], df[y])[0,1]\n",
    "\n",
    "chain = sim_chain()\n",
    "fork = sim_fork()\n",
    "coll = sim_collider()\n",
    "\n",
    "print(\"Chain: corr(A,C)  (marginal) =\", corr(\"A\",\"C\", chain))\n",
    "print(\"Fork:  corr(A,C)  (marginal) =\", corr(\"A\",\"C\", fork))\n",
    "print(\"Collider: corr(A,C) (marginal) =\", corr(\"A\",\"C\", coll))\n",
    "\n",
    "# Conditioning effects\n",
    "def partial_corr_xy_given_z(x,y,z,df, bins=10):\n",
    "    # Approximate partial correlation by binning on z (simple classroom-friendly approach).\n",
    "    df2 = df.copy()\n",
    "    df2[\"_zb\"] = pd.qcut(df2[z], q=bins, duplicates=\"drop\")\n",
    "    vals = []\n",
    "    for _,grp in df2.groupby(\"_zb\", observed=True):\n",
    "        if len(grp)>5:\n",
    "            vals.append(np.corrcoef(grp[x], grp[y])[0,1])\n",
    "    return np.nanmean(vals)\n",
    "\n",
    "print(\"\\nConditioning (approx via binning):\")\n",
    "print(\"Chain: corr(A,C | B) ≈\", partial_corr_xy_given_z(\"A\",\"C\",\"B\", chain))\n",
    "print(\"Fork:  corr(A,C | U) ≈\", partial_corr_xy_given_z(\"A\",\"C\",\"U\", fork))\n",
    "print(\"Collider: corr(A,C | B) ≈\", partial_corr_xy_given_z(\"A\",\"C\",\"B\", coll))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac85f572",
   "metadata": {},
   "source": [
    "### Interpretation of Results\n",
    "\n",
    "| Structure | Marginal Corr(A, C) | Conditional Corr(A, C \\| Z) | What it shows |\n",
    "|------------|--------------------:|-----------------------------:|----------------|\n",
    "| **Chain** | 0.58 | 0.05 | Conditioning on the mediator **B** blocks the flow from A → B → C. |\n",
    "| **Fork** | 0.50 | 0.04 | Conditioning on the confounder **U** removes the common-cause association. |\n",
    "| **Collider** | −0.01 | −0.47 | Conditioning on **B** (a common effect) creates a spurious link — classic *collider bias*. |\n",
    "\n",
    "**Summary:**  \n",
    "- **Chain & Fork:** conditioning *reduces* correlation (closes the path).  \n",
    "- **Collider:** conditioning *induces* correlation (opens a blocked path).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe43fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is a collider correlation always a negative value? Was this just a coincidence with the random seed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1b3183",
   "metadata": {},
   "source": [
    "\n",
    "## C) **Proxy variable** for an unobserved confounder\n",
    "\n",
    "Unobserved `U` (true smoking exposure) affects both `YellowTeeth (Z)` and `Cancer (Y)`;  \n",
    "`Smoking (X)` is noisy self-report we can't rely on for percision issues. Nicotin level in body measured accurately `NL` serves as a **proxy** for `U`.\n",
    "\n",
    "We compare naive estimate `P(Y|X)` with adjustment by the proxy `NL` (back-door via proxy).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "446d4a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Y (cancer) using self reported smoking only\n",
      "Naive (Y ~ X):\n",
      "  beta_X = 0.579\n",
      "\n",
      "Proxy-adjusted to include both self report and Nicotin level biomarker(Y ~ X + NL):\n",
      "  beta_X  = 0.006   (should shrink toward 0)\n",
      "  beta_NL = 1.356  (captures the true U effect)\n",
      "\n",
      "Predicting Y (cancer) using Biomarker only (Y ~ NL):\n",
      "  beta_NL = 1.361\n"
     ]
    }
   ],
   "source": [
    "# --- Section C (revised): Proxy variable with accurate biomarker NL ---\n",
    "\n",
    "N = 200_000\n",
    "rng = np.random.default_rng(12)\n",
    "\n",
    "# Unobserved true exposure\n",
    "U = rng.normal(0, 1, N)                 # unobserved driver of risk\n",
    "\n",
    "# Observed variables\n",
    "X  = U + rng.normal(0, 1.0, N)          # self-report (noisy, low precision)\n",
    "Z  = U + rng.normal(0, 0.8, N)          # yellow teeth (crude indicator; we won't use it for adjustment here)\n",
    "NL = U + rng.normal(0, 0.1, N)          # biomarker (accurate proxy; low noise)\n",
    "\n",
    "# Outcome depends on TRUE exposure (U), not X directly\n",
    "logit = -0.7 + 1.4 * U\n",
    "pY = 1 / (1 + np.exp(-logit))\n",
    "Y = (rng.random(N) < pY).astype(int)\n",
    "\n",
    "dfp = pd.DataFrame(dict(X=X, Z=Z, NL=NL, Y=Y))\n",
    "\n",
    "# --- Models: naive vs proxy-adjusted ---\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 1) Naive: Y ~ X  (confounded by U)\n",
    "m_naive = sm.Logit(dfp[\"Y\"], sm.add_constant(dfp[[\"X\"]])).fit(disp=False)\n",
    "\n",
    "# 2) Proxy-adjusted with accurate biomarker: Y ~ X + NL\n",
    "m_proxy = sm.Logit(dfp[\"Y\"], sm.add_constant(dfp[[\"X\",\"NL\"]])).fit(disp=False)\n",
    "\n",
    "# 3) Biomarker only: Y ~ NL  (close to the \"oracle\" using U)\n",
    "m_biomarker = sm.Logit(dfp[\"Y\"], sm.add_constant(dfp[[\"NL\"]])).fit(disp=False)\n",
    "\n",
    "print(\"Predicting Y (cancer) using self reported smoking only\")\n",
    "print(\"Naive (Y ~ X):\")\n",
    "print(f\"  beta_X = {m_naive.params['X']:.3f}\")\n",
    "\n",
    "print(\"\\nProxy-adjusted to include both self report and Nicotin level biomarker(Y ~ X + NL):\")\n",
    "print(f\"  beta_X  = {m_proxy.params['X']:.3f}   (should shrink toward 0)\")\n",
    "print(f\"  beta_NL = {m_proxy.params['NL']:.3f}  (captures the true U effect)\")\n",
    "\n",
    "print(\"\\nPredicting Y (cancer) using Biomarker only (Y ~ NL):\")\n",
    "print(f\"  beta_NL = {m_biomarker.params['NL']:.3f}\")\n",
    "\n",
    "# (Optional instructor check — uncomment to peek at \"truth\")\n",
    "# corr_U_X  = np.corrcoef(U, X)[0,1]\n",
    "# corr_U_Z  = np.corrcoef(U, Z)[0,1]\n",
    "# corr_U_NL = np.corrcoef(U, NL)[0,1]\n",
    "# print(f\"\\n[Hidden truth] corr(U,X)={corr_U_X:.2f}, corr(U,Z)={corr_U_Z:.2f}, corr(U,NL)={corr_U_NL:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049a5454",
   "metadata": {},
   "source": [
    "\n",
    "> **Observation:** With only `X` we pick up confounding from `U`.  \n",
    "> Adding the proxy `Z` absorbs much of `U`'s influence and moves `beta_X` toward the *direct* effect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc3d31b",
   "metadata": {},
   "source": [
    "## Excersice:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1cfad8",
   "metadata": {},
   "source": [
    "Secion A) In the parameterization `P(D|SA=0)` and `P(D|do(SA=0))` can both be close to 1.  \n",
    "What *qualitatively* changes between the two worlds? Explain using a one-sentence reference to the DAG.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae08ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this scenario, for both P(D|SA=0) and P(D|do(SA=0) to be close to 1, we would need to \n",
    "#ensure that Squad B will always follow the Captain's order and has a lethality of 1.0 as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2d2e41",
   "metadata": {},
   "source": [
    "\n",
    "## E) Quick Tasks (for credit / discussion)\n",
    "\n",
    "1. **Parameter flip:** In the firing squad model, change `p_kill_A` to 0.6 and `p_kill_B` to 0.99.  \n",
    "   - Re-run and record `P(D|SA=1)` vs `P(D|do(SA=1))`.  \n",
    "   - Explain in one sentence which way confounding moves the observational estimate.\n",
    "\n",
    "2. **Collider bias:** In section B, filter to the top 10% of `B` values in the collider model and compute `corr(A,C)` there.  \n",
    "   - Why does this selection amplify the association?\n",
    "\n",
    "3. **Proxy strength:** In section C, increase proxy noise (e.g., `Z = U + 1.5*eps_z`).  \n",
    "   - How do `beta_X` and `beta_Z` change? What does this say about **weak proxies**?\n",
    "\n",
    "4. **Back-door bins:** In section D, increase the number of `U` bins from 10 to 30.  \n",
    "   - Does the adjusted estimate stabilize? Why / why not?\n",
    "\n",
    "> *Tip:* Add a new cell under each section to keep your answers and code together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743112ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faf4d98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observational world:\n",
      "P(D=1 | SA=0) = 0.990  [n=52415]  (seeing)\n",
      "\n",
      "Interventional world:\n",
      "P(D=1 | do(SA=0)) = 0.470  [n=100000]  (doing)\n"
     ]
    }
   ],
   "source": [
    "N = 100000\n",
    "rng = np.random.default_rng(6)\n",
    "\n",
    "# Structural equations (binary)\n",
    "C = rng.binomial(1, 0.5, size=N)  # Captain order\n",
    "SA = (C & (rng.random(N) < 0.95)).astype(int)  # Squad A obeys if ordered\n",
    "SB = (C & (rng.random(N) < 0.95)).astype(int)  # Squad B obeys if ordered\n",
    "\n",
    "# Lethality (set to 1 for simplicity, adjust if you want realism)\n",
    "p_kill_A, p_kill_B = 0.6, 0.99\n",
    "hit_A = p_kill_A\n",
    "hit_B = p_kill_B\n",
    "# hit_A = (SA & (rng.random(N) < p_kill_A)).astype(int)\n",
    "# hit_B = (SB & (rng.random(N) < p_kill_B)).astype(int)\n",
    "D = np.maximum(hit_A, hit_B)\n",
    "\n",
    "# Combine all simulated variables into a single DataFrame for easier analysis and plotting\n",
    "obs_df = pd.DataFrame(dict(C=C, SA=SA, SB=SB, D=D))\n",
    "\n",
    "print(\"Observational world:\")\n",
    "# Compute P(D=1 | SA=0)\n",
    "p_obs = obs_df.loc[obs_df[\"SA\"] == 0, \"D\"].mean()\n",
    "n_obs = (obs_df[\"SA\"] == 0).sum()\n",
    "print(f\"P(D=1 | SA=0) = {p_obs:.3f}  [n={n_obs}]  (seeing)\")\n",
    "\n",
    "# --- Interventional: do(SA=0) ---\n",
    "C2 = rng.binomial(1, 0.5, size=N)                   # captain as before\n",
    "SA2 = np.zeros(N, dtype=int)                        # force A not to fire\n",
    "SB2 = (C2 & (rng.random(N) < 0.95)).astype(int)     # B still reacts to captain\n",
    "\n",
    "hit_A2 = (SA2 & (rng.random(N) < p_kill_A)).astype(int)\n",
    "hit_B2 = (SB2 & (rng.random(N) < p_kill_B)).astype(int)\n",
    "D2 = np.maximum(hit_A2, hit_B2)\n",
    "\n",
    "do_df = pd.DataFrame(dict(C=C2, SA=SA2, SB=SB2, D=D2))\n",
    "p_do = do_df[\"D\"].mean()\n",
    "n_do = len(do_df)\n",
    "\n",
    "print(\"\\nInterventional world:\")\n",
    "print(f\"P(D=1 | do(SA=0)) = {p_do:.3f}  [n={n_do}]  (doing)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d374de3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we changed the lethality probabilities, we see that given Squad A does not fire, \n",
    "#the probability of death is directly related to the lethality of Squad B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7207246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4387df71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain: corr(A,C)  (marginal) = 0.5801684286913067\n",
      "Fork:  corr(A,C)  (marginal) = 0.49713950788544\n",
      "Collider: corr(A,C) (marginal) = -0.01064898968845041\n",
      "\n",
      "Conditioning (approx via binning):\n",
      "Chain: corr(A,C | B) ≈ 0.047197088807209964\n",
      "Fork:  corr(A,C | U) ≈ 0.03878851215357192\n",
      "Collider: corr(A,C | B) ≈ -0.9205779788781971\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def sim_chain(N=50_000, seed=1):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    A = rng.normal(0,1,N)\n",
    "    B = A + rng.normal(0,1,N)\n",
    "    C = B + rng.normal(0,1,N)\n",
    "    return pd.DataFrame(dict(A=A,B=B,C=C))\n",
    "\n",
    "def sim_fork(N=50_000, seed=2):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    U = rng.normal(0,1,N)\n",
    "    A = U + rng.normal(0,1,N)\n",
    "    C = U + rng.normal(0,1,N)\n",
    "    return pd.DataFrame(dict(U=U,A=A,C=C))\n",
    "\n",
    "def sim_collider(N=50_000, seed=3):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    A = rng.normal(0,1,N)\n",
    "    C = rng.normal(0,1,N)\n",
    "    B = (A + C + rng.normal(0,1,N) * .1)  # collider\n",
    "    return pd.DataFrame(dict(A=A,B=B,C=C))\n",
    "\n",
    "def corr(x,y,df):\n",
    "    return np.corrcoef(df[x], df[y])[0,1]\n",
    "\n",
    "chain = sim_chain()\n",
    "fork = sim_fork()\n",
    "coll = sim_collider()\n",
    "\n",
    "print(\"Chain: corr(A,C)  (marginal) =\", corr(\"A\",\"C\", chain))\n",
    "print(\"Fork:  corr(A,C)  (marginal) =\", corr(\"A\",\"C\", fork))\n",
    "print(\"Collider: corr(A,C) (marginal) =\", corr(\"A\",\"C\", coll))\n",
    "\n",
    "# Conditioning effects\n",
    "def partial_corr_xy_given_z(x,y,z,df, bins=10):\n",
    "    # Approximate partial correlation by binning on z (simple classroom-friendly approach).\n",
    "    df2 = df.copy()\n",
    "    df2[\"_zb\"] = pd.qcut(df2[z], q=bins, duplicates=\"drop\")\n",
    "    vals = []\n",
    "    for _,grp in df2.groupby(\"_zb\", observed=True):\n",
    "        if len(grp)>5:\n",
    "            vals.append(np.corrcoef(grp[x], grp[y])[0,1])\n",
    "    return np.nanmean(vals)\n",
    "\n",
    "print(\"\\nConditioning (approx via binning):\")\n",
    "print(\"Chain: corr(A,C | B) ≈\", partial_corr_xy_given_z(\"A\",\"C\",\"B\", chain))\n",
    "print(\"Fork:  corr(A,C | U) ≈\", partial_corr_xy_given_z(\"A\",\"C\",\"U\", fork))\n",
    "print(\"Collider: corr(A,C | B) ≈\", partial_corr_xy_given_z(\"A\",\"C\",\"B\", coll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7ca463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corr(A,C | B) is amplified in this case because we are taking a much smaller sample size \n",
    "#of data that is conditioned on B, which creates a stronger observed correlation between A and C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d0bf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a30834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
